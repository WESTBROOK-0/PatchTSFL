[Info] parameters: Namespace(CSCM='Bottleneck_Construct', batch_size=32, covariate_size=4, d_bottleneck=128, d_inner_hid=512, d_k=128, d_model=512, d_v=128, data='ETTh1', data_path='ETTh1.csv', dec_in=7, decoder='FC', dropout=0.05, embed_type='DataEmbedding', enc_in=7, epoch=5, eval=False, hard_sample_mining=False, inner_size=5, input_size=96, inverse=False, iter_num=1, lr=0.0001, lr_step=0.1, model='Pyraformer', n_head=6, n_layer=4, predict_step=720, pretrain=False, root_path='../dataset/', seq_num=1, truncate=False, use_tvm=False, window_size=[4, 4, 4])
[Info] Number of parameters: 19069184
train 7825
test 2161
[ Epoch 1 ]
  - (Training) MSE:  0.53038elapse: 0.100 min
(2161, 720, 7)
test shape:(2161, 720, 7)
Epoch 0, mse:0.9506556391716003, mae:0.7675528526306152, rmse:0.9750156998634338, mape:21.3360538482666, mspe:164556.5
[ Epoch 2 ]
  - (Training) MSE:  0.39729elapse: 0.101 min
(2161, 720, 7)
test shape:(2161, 720, 7)
Epoch 1, mse:0.9600438475608826, mae:0.7766456007957458, rmse:0.9798182845115662, mape:23.22227668762207, mspe:192689.84375
[ Epoch 3 ]
  - (Training) MSE:  0.38785elapse: 0.098 min
(2161, 720, 7)
test shape:(2161, 720, 7)
Epoch 2, mse:0.9586098790168762, mae:0.7750052213668823, rmse:0.9790862202644348, mape:23.05189323425293, mspe:191124.375
[ Epoch 4 ]
  - (Training) MSE:  0.38588elapse: 0.099 min
(2161, 720, 7)
test shape:(2161, 720, 7)
Epoch 3, mse:0.9590650796890259, mae:0.7756925821304321, rmse:0.9793186783790588, mape:22.92388916015625, mspe:189218.8125
[ Epoch 5 ]
  - (Training) MSE:  0.38623elapse: 0.097 min
(2161, 720, 7)
test shape:(2161, 720, 7)
Epoch 4, mse:0.9576479196548462, mae:0.775529682636261, rmse:0.9785948991775513, mape:22.94495391845703, mspe:189247.328125
Iteration best metrics: [0.9506556391716003, 0.7675528526306152, 0.9750156998634338, 21.3360538482666, 164556.5]
Average Metrics: [9.50655639e-01 7.67552853e-01 9.75015700e-01 2.13360538e+01
 1.64556500e+05]
