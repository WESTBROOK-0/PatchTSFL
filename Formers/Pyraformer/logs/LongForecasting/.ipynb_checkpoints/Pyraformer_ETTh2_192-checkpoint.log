[Info] parameters: Namespace(CSCM='Bottleneck_Construct', batch_size=32, covariate_size=4, d_bottleneck=128, d_inner_hid=512, d_k=128, d_model=512, d_v=128, data='ETTh2', data_path='ETTh2.csv', dec_in=7, decoder='FC', dropout=0.05, embed_type='DataEmbedding', enc_in=7, epoch=5, eval=False, hard_sample_mining=False, inner_size=3, input_size=96, inverse=False, iter_num=1, lr=0.0001, lr_step=0.1, model='Pyraformer', n_head=6, n_layer=4, predict_step=192, pretrain=False, root_path='../dataset/', seq_num=1, truncate=False, use_tvm=False, window_size=[4, 4, 4])
[Info] Number of parameters: 11499776
train 8353
test 2689
[ Epoch 1 ]
  - (Training) MSE:  0.46999elapse: 0.094 min
(2689, 192, 7)
test shape:(2689, 192, 7)
Epoch 0, mse:4.1378350257873535, mae:1.6253713369369507, rmse:2.0341668128967285, mape:12.162206649780273, mspe:8028.791015625
[ Epoch 2 ]
  - (Training) MSE:  0.32071elapse: 0.093 min
(2689, 192, 7)
test shape:(2689, 192, 7)
Epoch 1, mse:4.897602558135986, mae:1.7870899438858032, rmse:2.213052749633789, mape:13.845765113830566, mspe:9322.123046875
[ Epoch 3 ]
  - (Training) MSE:  0.30744elapse: 0.101 min
(2689, 192, 7)
test shape:(2689, 192, 7)
Epoch 2, mse:5.016324043273926, mae:1.8121739625930786, rmse:2.239715099334717, mape:14.112607955932617, mspe:9594.17578125
[ Epoch 4 ]
  - (Training) MSE:  0.30623elapse: 0.091 min
(2689, 192, 7)
test shape:(2689, 192, 7)
Epoch 3, mse:5.150508403778076, mae:1.8391215801239014, rmse:2.269473075866699, mape:14.228082656860352, mspe:9732.943359375
[ Epoch 5 ]
  - (Training) MSE:  0.30620elapse: 0.091 min
(2689, 192, 7)
test shape:(2689, 192, 7)
Epoch 4, mse:5.007821559906006, mae:1.8113605976104736, rmse:2.237816333770752, mape:14.037101745605469, mspe:9514.9462890625
Iteration best metrics: [4.1378350257873535, 1.6253713369369507, 2.0341668128967285, 12.162206649780273, 8028.791015625]
Average Metrics: [4.13783503e+00 1.62537134e+00 2.03416681e+00 1.21622066e+01
 8.02879102e+03]
