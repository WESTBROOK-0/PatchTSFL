[Info] parameters: Namespace(CSCM='Bottleneck_Construct', batch_size=32, covariate_size=4, d_bottleneck=128, d_inner_hid=512, d_k=128, d_model=512, d_v=128, data='ETTh2', data_path='ETTh2.csv', dec_in=7, decoder='FC', dropout=0.05, embed_type='DataEmbedding', enc_in=7, epoch=5, eval=False, hard_sample_mining=False, inner_size=5, input_size=96, inverse=False, iter_num=1, lr=0.0001, lr_step=0.1, model='Pyraformer', n_head=6, n_layer=4, predict_step=720, pretrain=False, root_path='../dataset/', seq_num=1, truncate=False, use_tvm=False, window_size=[4, 4, 4])
[Info] Number of parameters: 19069184
train 7825
test 2161
[ Epoch 1 ]
  - (Training) MSE:  0.52087elapse: 0.102 min
(2161, 720, 7)
test shape:(2161, 720, 7)
Epoch 0, mse:4.27426290512085, mae:1.75022292137146, rmse:2.0674290657043457, mape:18.348970413208008, mspe:20958.603515625
[ Epoch 2 ]
  - (Training) MSE:  0.37160elapse: 0.099 min
(2161, 720, 7)
test shape:(2161, 720, 7)
Epoch 1, mse:3.961716651916504, mae:1.6804451942443848, rmse:1.9904061555862427, mape:17.648427963256836, mspe:18273.708984375
[ Epoch 3 ]
  - (Training) MSE:  0.36053elapse: 0.100 min
(2161, 720, 7)
test shape:(2161, 720, 7)
Epoch 2, mse:3.961251735687256, mae:1.6796188354492188, rmse:1.990289330482483, mape:17.688621520996094, mspe:18543.841796875
[ Epoch 4 ]
  - (Training) MSE:  0.35764elapse: 0.103 min
(2161, 720, 7)
test shape:(2161, 720, 7)
Epoch 3, mse:3.9102516174316406, mae:1.6704994440078735, rmse:1.97743558883667, mape:17.40020751953125, mspe:17809.212890625
[ Epoch 5 ]
  - (Training) MSE:  0.35684elapse: 0.099 min
(2161, 720, 7)
test shape:(2161, 720, 7)
Epoch 4, mse:3.949732542037964, mae:1.6774779558181763, rmse:1.9873933792114258, mape:17.652320861816406, mspe:18528.357421875
Iteration best metrics: [3.9102516174316406, 1.6704994440078735, 1.97743558883667, 17.40020751953125, 17809.212890625]
Average Metrics: [3.91025162e+00 1.67049944e+00 1.97743559e+00 1.74002075e+01
 1.78092129e+04]
